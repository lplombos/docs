---
title: Workflow Evalutaion
description: ''
---

## Workflow Evaluation

Evaluating the Workflow generated by miners is key to have a complete pipeline. This will be the necessary feedback that will make this subnet generate correct workflows that actually solve the userâ€™s requests. Even though we could use reasoning and logic approaches, we believe the only real way of evaluating a workflow is to execute the Workflow in a virtual browser environment.

As you may have noticed, workflows are agnostic in terms of where should the actions/steps be executed. Workflows just describe and state a series of actions from a predetermined finite list of actions that must be executed. These actions, depending on the environment that will execute them, will need a different implementation. For example, a simple action of clicking or navigating will take different forms if we are using a tool like Pupetter (Javascript) than when using Seelenium (python). We could even interact with a browser directly using the Chromium API.

The execution of this Workflow actions will have to be supervised by an AI process that will try to solve any possible error that arises during the execution and that will eventually evaluate (given defined criteria) how accurate was the workflow. We can combine this with quantitative metrics that take into account correct actions, errors, total iterations till success, etc.

Here we could choose from different criteria to evaluate. Just giving score to those workflows that are completely correct or to also give score to technically wrong workflows that could be solved in the execution phase by doing little adjustments. This scoring criteria will be key and will dictate how miners operate. All this while keeping validator code light and fast enough to handle lots of miners simultaneously.
